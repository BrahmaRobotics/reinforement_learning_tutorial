{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 值迭代算法\n",
    "作者：stzhao\n",
    "github: https://github.com/zhaoshitian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、定义环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys,os\n",
    "curr_path = os.path.abspath('')\n",
    "parent_path = os.path.dirname(curr_path)\n",
    "sys.path.append(parent_path)\n",
    "from envs.simple_grid import DrunkenWalkEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env,seed = 1):\n",
    "    ## 这个函数主要是为了固定随机种子\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import os\n",
    "    env.seed(seed) \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = DrunkenWalkEnv(map_name=\"theAlley\")\n",
    "env = DrunkenWalkEnv(map_name=\"4x4\")\n",
    "all_seed(env, seed = 1) # 设置随机种子为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "\n",
    "def visualize_env(env):\n",
    "    # 获取地图\n",
    "    desc = env.desc.tolist()\n",
    "    desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "    \n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    \n",
    "    # 设置颜色映射\n",
    "    colors = ['white', 'gray', 'green', 'red']  # 对应 '.', 'H', 'S', 'G'\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # 创建数值矩阵\n",
    "    value_map = {\n",
    "        '.': 0,\n",
    "        'H': 1,\n",
    "        'S': 2,\n",
    "        'G': 3\n",
    "    }\n",
    "    \n",
    "    # 转换为数值矩阵\n",
    "    grid = np.array([[value_map[cell] for cell in row] for row in desc])\n",
    "    \n",
    "    # 绘制网格\n",
    "    im = ax.imshow(grid, cmap=cmap, aspect='equal')\n",
    "    \n",
    "    # 添加文字标注\n",
    "    for i in range(len(desc)):\n",
    "        for j in range(len(desc[0])):\n",
    "            text = desc[i][j]\n",
    "            ax.text(j, i, text, ha='center', va='center', color='black')\n",
    "    \n",
    "    # 设置轴标签\n",
    "    ax.set_xticks(range(len(desc[0])))\n",
    "    ax.set_yticks(range(len(desc)))\n",
    "    \n",
    "    # 添加标题\n",
    "    plt.title('醉汉走路环境', fontsize=12)\n",
    "    \n",
    "    # 添加图例\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, label=label)\n",
    "                      for color, label in [\n",
    "                          ('green', '起点 (S)'),\n",
    "                          ('red', '终点 (G)'),\n",
    "                          ('gray', '坑洞 (H)'),\n",
    "                          ('white', '正常路面 (.)')\n",
    "                      ]]\n",
    "    ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例\n",
    "env = DrunkenWalkEnv(map_name=\"4x4\")\n",
    "visualize_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、价值迭代算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.005, discount_factor=0.9, max_count = 100):\n",
    "    Q = np.zeros((env.nS, env.nA)) # 初始化一个Q表格\n",
    "    count = 0\n",
    "    deltas = []  # 记录每次迭代的delta值\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        Q_tmp = np.zeros((env.nS, env.nA))\n",
    "        for state in range(env.nS):\n",
    "            for a in range(env.nA):\n",
    "                accum = 0.0\n",
    "                reward_total = 0.0\n",
    "                for prob, next_state, reward, done in env.P[state][a]:\n",
    "                    accum += prob* np.max(Q[next_state, :])\n",
    "                    reward_total += prob * reward\n",
    "                Q_tmp[state, a] = reward_total + discount_factor * accum\n",
    "                delta = max(delta, abs(Q_tmp[state, a] - Q[state, a]))\n",
    "        Q = Q_tmp.copy()\n",
    "        deltas.append(delta)\n",
    "        count += 1\n",
    "        if delta < theta or count > max_count: # 这里设置了即使算法没有收敛，跑100次也退出循环\n",
    "            print(f\"算法在第 {count} 次迭代后收敛\")\n",
    "            break \n",
    "    # 绘制收敛过程\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(deltas)\n",
    "    plt.xlabel('迭代次数')\n",
    "    plt.ylabel('最大变化量 (delta)')\n",
    "    plt.title('价值迭代收敛过程')\n",
    "    plt.yscale('log')  # 使用对数尺度更容易观察\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = value_iteration(env, theta=0.005, discount_factor=0.9, max_count=100)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros([env.nS, env.nA]) # 初始化一个策略表格\n",
    "for state in range(env.nS):\n",
    "    best_action = np.argmax(Q[state, :]) #根据价值迭代算法得到的Q表格选择出策略\n",
    "    policy[state, best_action] = 1\n",
    "\n",
    "policy = [int(np.argwhere(policy[i]==1)) for i in range(env.nS) ]\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 1000 # 测试1000次\n",
    "\n",
    "def test(env,policy):\n",
    "    \n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    success = []  # 记录该回合是否成功走到终点\n",
    "    for i_ep in range(num_episode):\n",
    "        ep_reward = 0  # 记录每个episode的reward\n",
    "        state = env.reset()  # 重置环境, 重新开一局（即开始新的一个回合） 这里state=0\n",
    "        while True:\n",
    "            action = policy[state]  # 根据算法选择一个动作\n",
    "            next_state, reward, done, _ = env.step(action)  # 与环境进行一个交互\n",
    "            state = next_state  # 更新状态\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        if state==15: # 即走到终点\n",
    "            success.append(1)\n",
    "        else:\n",
    "            success.append(0)\n",
    "        rewards.append(ep_reward)\n",
    "    acc_suc = np.array(success).sum()/num_episode\n",
    "    print(\"测试的成功率是：\", acc_suc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(env, policy)\n",
    "# 打印策略\n",
    "print(\"\\n最优策略：\")\n",
    "for i in range(env.nrow):\n",
    "    row = \"\"\n",
    "    for j in range(env.ncol):\n",
    "        state = i * env.ncol + j\n",
    "        action = policy[state]\n",
    "        if env.desc[i][j] in b'GH':\n",
    "            row += \" * \"  # 目标或坑洞\n",
    "        else:\n",
    "            row += \" \" + [\"←\", \"↓\", \"→\", \"↑\"][action] + \" \"\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88a829278351aa402b7d6303191a511008218041c5cfdb889d81328a3ea60fbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
